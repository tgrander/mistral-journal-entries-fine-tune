# Treysifer's Persona Evolution Project

Welcome to the repository for a groundbreaking exploration into AI's potential to understand, adopt, and evolve with a human's writing style and persona growth. This project focuses on the journey of "Treysifer," a character whose decade-long development has been captured through personal journal entries and reflected through an advanced AI model.

## Project Overview

The core of this project is the fine-tuning of an open-source Mistral model with personal journal entries spanning over ten years. The aim was to see how well the AI could adopt the persona, voice, and nuanced writing style of the character Treysifer, as well as understand and depict the character's growth over time.

## Results

The experiment yielded spectacular results, demonstrating the model's proficiency in capturing the essence of Treysifer's evolution. It showcases the potential of AI in personal storytelling, character development, and perhaps, understanding the complexities of human growth and change over time.

## Disclaimer

Due to the personal nature of the journal entries used for training, the model and the data cannot be shared publicly. However, this repository aims to provide an overview of the project, the approach taken, and the insights gained.

## Objectives

- **Model Training:** Details on how the Mistral model was fine-tuned with personal data.
- **Character Development:** Insights into how the AI understood and evolved with Treysifer's character over a decade.
- **AI and Personal Storytelling:** Exploration of AI's potential in personal storytelling and character analysis.

## Future Work

While the project has already provided fascinating outcomes, there's much more to explore. Future directions could include:

- Investigating other AI models and their ability to capture personal growth and character development.
- Exploring the ethical implications of using personal data for AI training.
- Enhancing AI's understanding of complex character emotions and evolution over longer periods.

## Contribution

As the project is based on highly personal data, direct contributions to model training are not feasible. However, discussions, suggestions for future research directions, and general insights into AI's role in storytelling are highly welcome.

## Contact

For inquiries, discussions, or collaborations, please feel free to reach out. treygranderson@gmail.com

Thank you for your interest in this unique exploration of AI and personal storytelling.

# Notebook Project Template

## Overview

This project template is designed for data science and analytics workflows using Jupyter Notebooks. It provides a structured and standardized way to organize code, data, and outputs for efficient and reproducible research.

## Structure

The template is organized into the following directories:

- **`data/`**: Contains raw and processed data.
  - `raw/`: Stores the original, unaltered data.
  - `processed/`: Holds data that has been cleaned, transformed, or otherwise processed.
- **`notebooks/`**: Contains Jupyter notebooks (.ipynb files) used for analysis and data processing.
- **`scripts/`**: For standalone Python scripts, often used for more complex or reusable code.
- **`utils/`**: Includes utility functions and helper scripts.
- **`outputs/`**: Stores the results and products of analyses.
  - `figures/`: For plots, charts, and other visualizations.
  - `data/`: Final or exported data sets, ready for sharing or publishing.
  - `logs/`: Log files for tracking and debugging.
  - `models/`: Trained machine learning model files.
  - `summaries/`: Textual output such as reports and summaries.
- **`env/`**: Virtual environment directory (not tracked by version control).

## Getting Started

1. **Set Up Environment**: 
   - Create a virtual environment: `python -m venv env`
   - Activate the environment:
     - Windows: `.\env\Scripts\activate`
     - Unix/macOS: `source env/bin/activate`
   - Install required packages: `pip install -r requirements.txt`

2. **Working with Notebooks**:
   - Jupyter notebooks are located in the `notebooks/` directory.
   - Start JupyterLab with `jupyter lab` and open notebooks from the interface.

3. **Using the Data Directory**:
   - Place your raw data in `data/raw/`.
   - Save processed data in `data/processed/`.

4. **Scripts and Utilities**:
   - Store reusable scripts in `scripts/`.
   - Place utility functions in `utils/`.

5. **Saving Outputs**:
   - Save figures and plots in `outputs/figures/`.
   - Export final data sets to `outputs/data/`.

6. **Logging**:
   - Generate and store log files in `outputs/logs/`.

## Best Practices

- Keep raw data immutable to maintain data integrity.
- Document each step in your Jupyter notebooks for clarity and reproducibility.
- Write modular and reusable code in scripts and utility functions.
- Regularly commit changes to version control.

---

This template provides a foundational structure to kickstart your notebook-based projects, ensuring that your work remains organized and adheres to best practices in data science.
